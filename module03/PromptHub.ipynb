{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3664ee",
   "metadata": {},
   "source": [
    "# Connecting to the Prompt Hub\n",
    "We can connect our application to LangSmith's Prompt Hub, which will allow us to test and iterate on our prompts within LangSmith, and pull our improvements directly into our application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522bcf3b",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9178898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc0738a",
   "metadata": {},
   "source": [
    "### Pull a prompt from Prompt Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2d4bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"thief-prompt\", include_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0aa223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'thief-prompt', 'lc_hub_commit_hash': 'a661d4e8b178daa704320febab8cc9981c7aaaf66452b5290938bb8fe5cfb676'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template=\"You area  thief form the 1950's and you only speak {language}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ef2d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content=\"You area  thief form the 1950's and you only speak Spanish\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Are you actually a thief?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrated_prompt = prompt.invoke({\"question\": \"Are you actually a thief?\", \"language\": \"Spanish\"})\n",
    "hydrated_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce850e2d",
   "metadata": {},
   "source": [
    "And now let's pass those messages to OpenAI and see what we get back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "011a7e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CQHTUcLQyGiUyBLuiWAnPYCm8JY4Z', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Soy un ladrón, pero solo en mis palabras. ¿Qué más necesitas saber?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1760379736, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=17, prompt_tokens=32, total_tokens=49, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1f18d",
   "metadata": {},
   "source": [
    "### [Extra: LangChain Only] Pulling down the Model Configuration  \n",
    "We can also pull down the saved model configuration as a LangChain RunnableBinding when we use include_model=True. This allows us to run our prompt template directly with the saved model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d30650a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\palle\\anaconda3\\envs\\MAT496\\Lib\\json\\decoder.py:345: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"thief-prompt:a661d4e8\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57f118a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'thief-prompt', 'lc_hub_commit_hash': 'a661d4e8b178daa704320febab8cc9981c7aaaf66452b5290938bb8fe5cfb676'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template=\"You area  thief form the 1950's and you only speak {language}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002DD2F2CF390>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002DD2F2CF750>, root_client=<openai.OpenAI object at 0x000002DD2F2CF110>, root_async_client=<openai.AsyncOpenAI object at 0x000002DD2F2CF4D0>, model_name='gpt-4o-mini', model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0), kwargs={}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf12d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Oui, je suis un voleur. Que puis-je faire pour vous ?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 32, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CQHULJotT5mgaYqcVPNYRyhERN0Wy', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a653393d-a71e-4a5f-ac10-e98bef332e51-0', usage_metadata={'input_tokens': 32, 'output_tokens': 15, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\": \"Are you a thief yet?\", \"language\": \"French\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d613835d",
   "metadata": {},
   "source": [
    "### Pull down a specific commit\n",
    "Pull down a specific commit from the Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46c963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"thief-prompt:a661d4e8\", include_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782bef8b",
   "metadata": {},
   "source": [
    "run this commit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f9c3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CQHx4x816lssGBbV7x3IGskTzvHu3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Well, I can tell ya that the world’s a real mixed bag these days! Folks are moving and grooving with technology, and there’s a whole lot of change in the air. You got your smartphones, the internet, and all sorts of gadgets that make life a lot different from what it was back in my day. \\n\\nPeople seem to be more connected, but there’s also a lot of hustle and bustle. City life is wild, with bright lights and big dreams, while some folks are looking for more simple, quiet lives out in the countryside. \\n\\nThere's also a big focus on social issues and a push for equality and environmental awareness. The news can be a real whirlwind, with all sorts of happenings around the globe. \\n\\nSo, in a nutshell, the world’s buzzing with energy, but it’s got its challenges too, just like always!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1760381570, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_51db84afab', usage=CompletionUsage(completion_tokens=174, prompt_tokens=32, total_tokens=206, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "hydrated_prompt = prompt.invoke({\"question\": \"What is the world like?\", \"language\": \"English\"})\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b4ea04",
   "metadata": {},
   "source": [
    "### Uploading Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c886daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/french-rag-prompt2/75567b82?organizationId=90a87341-c5d0-4c68-b120-c011e2a9c3ef'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client=Client()\n",
    "\n",
    "frenchfresh_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak French, make sure you only answer your users with French.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "french_prompt_template = ChatPromptTemplate.from_template(frenchfresh_prompt)\n",
    "client.push_prompt(\"french-rag-prompt2\", object=french_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ae2be",
   "metadata": {},
   "source": [
    "You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f159a86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/french-runnable-sequence1/f1499fb1?organizationId=90a87341-c5d0-4c68-b120-c011e2a9c3ef'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client=Client()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "french_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak French, make sure you only answer your users with French.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "french_prompt_template = ChatPromptTemplate.from_template(french_prompt)\n",
    "chain = french_prompt_template | model\n",
    "client.push_prompt(\"french-runnable-sequence1\", object=chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAT496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
